<h2>Task 1: Dialogue Summarization with FLAN-T5</h2>

<p><strong>Overview:</strong> This task involved summarizing dialogues from a dataset named "Dialogue sum" using the FLAN-T5 model. Our approach included a variety of techniques aimed at improving the model's ability to generate concise summaries from conversations.</p>

<ul>
  <li><strong>Environment Setup:</strong> We began by setting up our environment, which included installing necessary Python libraries such as PyTorch, Torch data, and Hugging Face Transformers.</li>
  <li><strong>Model and Tokenizer:</strong> The FLAN-T5 model, along with its tokenizer, was loaded from the Hugging Face Transformers library to facilitate our summarization tasks.</li>
  <li><strong>Initial Attempts:</strong> Our initial attempts to summarize dialogues did not yield satisfactory results, prompting us to explore various prompting techniques.</li>
  <li><strong>Prompt Engineering:</strong> We experimented with different prompts, including zero-shot inference and in-context learning, to refine the summarization outputs.</li>
  <li><strong>Inference Techniques:</strong> Techniques such as one-shot and few-shot inference were explored, providing the model with examples to improve its performance.</li>
  <li><strong>Parameter Tuning:</strong> The impact of configuration parameters, like sampling temperature, on model responses was investigated. Adjusting these parameters can lead to more varied and creative responses.</li>
  <li><strong>Objective:</strong> The overarching goal was to familiarize users with the nuances of prompt engineering and model fine-tuning for enhanced summarization results.</li>
</ul>

<p>By exploring different techniques and fine-tuning the model, we aimed to achieve more accurate and concise summaries of dialogues, enhancing the model's utility for real-world applications.</p>


<h2>Lab 2: Fine-Tuning FLAN-T5 with PEFT and LoRA</h2>

<p><strong>Objective:</strong> The primary goal of this Task is to enhance the summarization capabilities of the FLAN-T5 model through full fine-tuning and Parameter-Efficient Fine-Tuning (PEFT), incorporating prompt instructions for improved performance.</p>

<ul>
  <li><strong>Setting Up:</strong> Similar to Task 1, we begin by setting up the environment and installing all the necessary libraries required for the task.</li>
  <li><strong>Data Preparation:</strong> The dataset is prepared and loaded along with the original FLAN-T5 model and tokenizer to facilitate the fine-tuning process.</li>
  <li><strong>Fine-Tuning Strategies:</strong> We compare different fine-tuning strategies, including full fine-tuning and PEFT, to determine their impact on the model's summarization ability.</li>
  <li><strong>Instruction Fine-Tuning:</strong> A convenience function is utilized to tokenize the dataset and wrap it with prompts for instruction-based fine-tuning.</li>
  <li><strong>Training Configuration:</strong> Training arguments are meticulously defined, specifying the learning rate, number of epochs, and steps for the fine-tuning process.</li>
  <li><strong>Qualitative Analysis:</strong> Summaries generated by both the original and fine-tuned models are examined to conduct a qualitative analysis of the improvements.</li>
  <li><strong>Quantitative Evaluation:</strong> ROUGE metrics are calculated to quantitatively assess the summarization performance of each model, offering a basis for comparison.</li>
  <li><strong>PEFT with LoRA:</strong> The process and benefits of PEFT are detailed, emphasizing how it reduces resource consumption while maintaining or improving performance.</li>
  <li><strong>Model Comparison:</strong> Sample prompts from the test dataset are used to evaluate both the instruction fine-tuned and PEFT models, with ROUGE metrics providing a comparative analysis of the models' performance.</li>
</ul>

<p>This Task offers a comprehensive exploration of fine-tuning strategies for the FLAN-T5 model, showcasing the significant potential of prompt engineering and parameter-efficient techniques in enhancing the model's ability for specific tasks like dialogue summarization.</p>


<h2>Task 3: Mitigating Toxicity with RLHF</h2>

<p><strong>Objective:</strong> The focus of Task 3 is to utilize Reinforcement Learning with Human Feedback (RLHF) to reduce the toxicity levels in the outputs of the instruction fine-tuned FLAN-T5 model developed in Lab 2.</p>

<ul>
  <li><strong>Introduction to RLHF:</strong> We start with an overview of RLHF, a cutting-edge technique designed to refine model outputs based on human evaluators' feedback, specifically targeting toxicity reduction in generated text.</li>
  <li><strong>Environment Setup:</strong> Necessary Python libraries such as PyTorch, transformers, datasets, evaluate, Peft (Parameter-Efficient Fine-Tuning), and trl (Textual Reinforcement Learning) are installed.</li>
  <li><strong>Model and Data Preparation:</strong> We load the instruction fine-tuned model from Lab 2 and prepare the dataset for RLHF, including tokenization and prompt wrapping techniques.</li>
  <li><strong>Toxicity Evaluation Model:</strong> A toxicity evaluation model (Facebook's RoBERTa model for hate speech detection) is used to classify texts and optimize the RLHF process towards generating non-toxic outputs.</li>
  <li><strong>Sentiment Analysis Pipeline:</strong> A sentiment analysis pipeline is established for simplifying text classification tasks, aiding in the overall detoxification effort.</li>
  <li><strong>Evaluation and Optimization:</strong> An evaluation mechanism calculates mean toxicity scores, guiding the reinforcement learning process to effectively reduce toxicity.</li>
  <li><strong>PPO Training:</strong> The PPOTrainer, initialized with the model from Lab 2, undergoes training to minimize toxicity through RLHF, adjusting outputs based on predefined human feedback.</li>
  <li><strong>Quantitative and Qualitative Analysis:</strong> Post-training, the model's performance is assessed through both quantitative toxicity scores and qualitative analysis of the generated text, demonstrating the effectiveness of the RLHF approach.</li>
</ul>

<p>Task3 3 showcases the practical application of RLHF in enhancing language models by reducing toxicity, highlighting the potential of such techniques in improving the safety and quality of AI-generated content.</p>

